{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e56b4b0a-fe33-44dd-9025-c3cb4b957d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataloader import DataLoader\n",
    "from transformers import DataCollatorWithPadding\n",
    "from datasets import load_dataset, Dataset, concatenate_datasets\n",
    "from transformers import AutoTokenizer\n",
    "import transformers\n",
    "import numpy as np\n",
    "import torch\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3dc2f8a5-6248-431f-9e7b-a7ee782d3c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FEED_FORWARD_DIM = 32\n",
    "# BATCH_SIZE       = 2\n",
    "# EMBED_DIM        = 300\n",
    "# NUM_HEADS        = 3\n",
    "# NUM_LAYERS       = 2\n",
    "# EPOCHS           = 3\n",
    "# NUM_TOKENS_TO_GENERATE = 80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0089ab3b-4f4c-4f2e-a300-4c82d8211c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_gpt = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\") \n",
    "# tokenizer_gpt = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "setfit = load_dataset(\"SetFit/bbc-news\")\n",
    "dataset = concatenate_datasets([setfit['train'], setfit['test']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6d63984-3865-4c7a-843f-921a236e4bd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertTokenizerFast(name_or_path='distilbert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_gpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aaeca5b9-3432-4e73-a55d-7be9e013d949",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE       = tokenizer_gpt.vocab_size\n",
    "SEQ_LEN          = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98371f80-e268-4d43-9758-44809328508c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def tokenize_function(example):\n",
    "#     return tokenizer_gpt(example[\"text\"], padding=True, truncation=True, max_length=SEQ_LEN)\n",
    "\n",
    "# tokenized_dataset = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca4c6f3e-6f34-471f-95c9-cb79cc6b81cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " 100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " 101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " 102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " 103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True)}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_gpt.added_tokens_decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df6d9ab6-1d08-48b8-be60-6e30ad98bff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window_inputs(text, max_length = 512):\n",
    "    \n",
    "    tokenized = tokenizer_gpt(text)\n",
    "    input_ids = tokenized['input_ids']\n",
    "    # attn_mask = tokenized['attention_mask']\n",
    "    input_ids.pop(0) # 101\n",
    "    input_ids.pop(-1) # 102\n",
    "\n",
    "    if len(input_ids) <= max_length-1:\n",
    "        return [ [101] + input_ids[:-1] + [102], input_ids[-1] ]\n",
    "    else:\n",
    "        data = []\n",
    "        \n",
    "        for i in range(len(input_ids)-max_length-1):\n",
    "            temp = input_ids[i:(i+max_length-1)]\n",
    "            \n",
    "            data.append(\n",
    "               [ [101] + temp[:-1] + [102], temp[-1] ] \n",
    "            )\n",
    "\n",
    "        return data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "039d8b26-0d62-4558-b7c2-c462152a0a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding_and_attn(arr_tup, max_length=512):\n",
    "    arr, _ = arr_tup\n",
    "    \n",
    "    ones = len(arr)\n",
    "    zeros = max_length - len(arr)\n",
    "    \n",
    "    if ones < max_length:\n",
    "        attn = [1 for _ in range(ones)] + [0 for _ in range(zeros)]\n",
    "        arr_pad = arr + [0 for _ in range(zeros)]\n",
    "        return arr_pad, attn \n",
    "    else: \n",
    "        return arr, [1 for _ in range(max_length)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7f95d17a-4882-4cac-b3fd-911a2564284b",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_data = {\n",
    "    'label': [],\n",
    "    'input_ids' : [], \n",
    "    'attention_mask' : []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "02eb761d-9a10-4f9e-b3e2-91bb8ce4b1db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (706 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(dataset)): \n",
    "    temp = sliding_window_inputs(dataset[i]['text'])\n",
    "    if len(temp) == 2 and isinstance(temp[1], int):\n",
    "        label = temp[1]\n",
    "        arr, attn = padding_and_attn(temp)\n",
    "        gpt_data['label'].append(label)\n",
    "        gpt_data['attention_mask'].append(attn)\n",
    "        gpt_data['input_ids'].append(arr)\n",
    "    else:\n",
    "        for t in temp: \n",
    "            label = t[1]\n",
    "            arr, attn = padding_and_attn(t)\n",
    "            gpt_data['label'].append(label)\n",
    "            gpt_data['attention_mask'].append(attn)\n",
    "            gpt_data['input_ids'].append(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "61609195-e19f-463d-9456-7aef028f0c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = Dataset.from_dict(gpt_data)\n",
    "# ds.set_format(type='torch', columns=['label', 'input_ids', 'attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1261c6c5-199e-4835-87d4-6f5dddc9e5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(\n",
    "    ds,\n",
    "    batch_size=30\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2a225ba0-9aa7-4518-82e1-7e97ea76a85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e83d91b4-6944-4645-86aa-febc0099c6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.stack(test['input_ids']).T\n",
    "label = test['label']\n",
    "attn = torch.stack(test['attention_mask']).T "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29addb00-94c2-4ebf-8c0c-f30886a26977",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2871c84e-4d83-4eda-af97-7d10969a5ee7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05370eae-b848-43da-82ae-3f13e3182d3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
